---------------------------------------------------------------------------
Training stage 0
Sampled 10911 windows from 14071 images.
Done sampling windows (time=380s).
Computing lambdas... done (time=94s).
Extracting features... done (time=22s).
Sampled 25000 windows from 1024 images.
Done sampling windows (time=38s).
Extracting features... done (time=25s).
Training AdaBoost: nWeak= 64 nFtrs=5120 pos=21822 neg=25000
 i=  16 alpha=1.000 err=0.204 loss=7.77e-03
 i=  32 alpha=1.000 err=0.208 loss=1.56e-04
 i=  48 alpha=1.000 err=0.200 loss=2.91e-06
 i=  64 alpha=1.000 err=0.184 loss=5.52e-08
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=30.6s).
Done training stage 0 (time=593s).
---------------------------------------------------------------------------
Training stage 1
Sampled 25000 windows from 1280 images.
Done sampling windows (time=277s).
Extracting features... done (time=25s).
Training AdaBoost: nWeak=256 nFtrs=5120 pos=21822 neg=50000
 i=  16 alpha=1.000 err=0.324 loss=1.77e-01
 i=  32 alpha=1.000 err=0.340 loss=6.68e-02
 i=  48 alpha=1.000 err=0.338 loss=2.56e-02
 i=  64 alpha=1.000 err=0.358 loss=9.82e-03
 i=  80 alpha=1.000 err=0.343 loss=3.70e-03
 i=  96 alpha=1.000 err=0.348 loss=1.41e-03
 i= 112 alpha=1.000 err=0.340 loss=5.24e-04
 i= 128 alpha=1.000 err=0.337 loss=1.98e-04
 i= 144 alpha=1.000 err=0.345 loss=7.51e-05
 i= 160 alpha=1.000 err=0.341 loss=2.85e-05
 i= 176 alpha=1.000 err=0.341 loss=1.04e-05
 i= 192 alpha=1.000 err=0.351 loss=4.01e-06
 i= 208 alpha=1.000 err=0.365 loss=1.49e-06
 i= 224 alpha=1.000 err=0.362 loss=5.57e-07
 i= 240 alpha=1.000 err=0.356 loss=2.07e-07
 i= 256 alpha=1.000 err=0.341 loss=7.40e-08
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=182.2s).
Done training stage 1 (time=487s).
---------------------------------------------------------------------------
Training stage 2
Sampled 25000 windows from 6080 images.
Done sampling windows (time=1376s).
Extracting features... done (time=42s).
Training AdaBoost: nWeak=1024 nFtrs=5120 pos=21822 neg=50000
 i=  16 alpha=1.000 err=0.350 loss=2.89e-01
 i=  32 alpha=1.000 err=0.368 loss=1.51e-01
 i=  48 alpha=1.000 err=0.378 loss=7.93e-02
 i=  64 alpha=1.000 err=0.372 loss=4.16e-02
 i=  80 alpha=1.000 err=0.383 loss=2.22e-02
 i=  96 alpha=1.000 err=0.375 loss=1.14e-02
 i= 112 alpha=1.000 err=0.376 loss=5.93e-03
 i= 128 alpha=1.000 err=0.391 loss=3.09e-03
 i= 144 alpha=1.000 err=0.368 loss=1.56e-03
 i= 160 alpha=1.000 err=0.374 loss=8.02e-04
 i= 176 alpha=1.000 err=0.368 loss=4.09e-04
 i= 192 alpha=1.000 err=0.378 loss=2.12e-04
 i= 208 alpha=1.000 err=0.380 loss=1.08e-04
 i= 224 alpha=1.000 err=0.368 loss=5.51e-05
 i= 240 alpha=1.000 err=0.369 loss=2.82e-05
 i= 256 alpha=1.000 err=0.367 loss=1.47e-05
 i= 272 alpha=1.000 err=0.376 loss=7.38e-06
 i= 288 alpha=1.000 err=0.381 loss=3.78e-06
 i= 304 alpha=1.000 err=0.375 loss=1.96e-06
 i= 320 alpha=1.000 err=0.362 loss=9.76e-07
 i= 336 alpha=1.000 err=0.357 loss=4.76e-07
 i= 352 alpha=1.000 err=0.371 loss=2.37e-07
 i= 368 alpha=1.000 err=0.360 loss=1.18e-07
 i= 384 alpha=1.000 err=0.369 loss=5.82e-08
 i= 400 alpha=1.000 err=0.371 loss=2.87e-08
 i= 416 alpha=1.000 err=0.356 loss=1.41e-08
 i= 432 alpha=1.000 err=0.371 loss=7.01e-09
 i= 448 alpha=1.000 err=0.371 loss=3.52e-09
 i= 464 alpha=1.000 err=0.372 loss=1.79e-09
 i= 480 alpha=1.000 err=0.362 loss=9.09e-10
 i= 496 alpha=1.000 err=0.365 loss=4.51e-10
 i= 512 alpha=1.000 err=0.373 loss=2.32e-10
 i= 528 alpha=1.000 err=0.357 loss=1.12e-10
 i= 544 alpha=1.000 err=0.363 loss=5.69e-11
 i= 560 alpha=1.000 err=0.371 loss=2.83e-11
 i= 576 alpha=1.000 err=0.378 loss=1.39e-11
 i= 592 alpha=1.000 err=0.368 loss=7.03e-12
 i= 608 alpha=1.000 err=0.358 loss=3.47e-12
 i= 624 alpha=1.000 err=0.368 loss=1.72e-12
 i= 640 alpha=1.000 err=0.379 loss=8.45e-13
 i= 656 alpha=1.000 err=0.361 loss=4.15e-13
 i= 672 alpha=1.000 err=0.372 loss=2.09e-13
 i= 688 alpha=1.000 err=0.377 loss=1.06e-13
 i= 704 alpha=1.000 err=0.374 loss=5.41e-14
 i= 720 alpha=1.000 err=0.382 loss=2.69e-14
 i= 736 alpha=1.000 err=0.377 loss=1.37e-14
 i= 752 alpha=1.000 err=0.357 loss=6.93e-15
 i= 768 alpha=1.000 err=0.364 loss=3.45e-15
 i= 784 alpha=1.000 err=0.380 loss=1.71e-15
 i= 800 alpha=1.000 err=0.368 loss=8.64e-16
 i= 816 alpha=1.000 err=0.365 loss=4.25e-16
 i= 832 alpha=1.000 err=0.369 loss=2.16e-16
 i= 848 alpha=1.000 err=0.366 loss=1.07e-16
 i= 864 alpha=1.000 err=0.361 loss=5.33e-17
 i= 880 alpha=1.000 err=0.363 loss=2.63e-17
 i= 896 alpha=1.000 err=0.370 loss=1.32e-17
 i= 912 alpha=1.000 err=0.364 loss=6.67e-18
 i= 928 alpha=1.000 err=0.371 loss=3.32e-18
 i= 944 alpha=1.000 err=0.368 loss=1.62e-18
 i= 960 alpha=1.000 err=0.384 loss=8.06e-19
 i= 976 alpha=1.000 err=0.361 loss=3.97e-19
 i= 992 alpha=1.000 err=0.364 loss=1.96e-19
 i=1008 alpha=1.000 err=0.378 loss=9.89e-20
 i=1024 alpha=1.000 err=0.383 loss=4.90e-20
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=3309.0s).
Done training stage 2 (time=4737s).
---------------------------------------------------------------------------
Training stage 3
Sampled 3224 windows from 14071 images.
Done sampling windows (time=28045s).
Extracting features... done (time=3s).
Training AdaBoost: nWeak=4096 nFtrs=5120 pos=21822 neg=50000
 i=  16 alpha=1.000 err=0.371 loss=3.27e-01
 i=  32 alpha=1.000 err=0.388 loss=1.79e-01
 i=  48 alpha=1.000 err=0.370 loss=1.00e-01
 i=  64 alpha=1.000 err=0.393 loss=5.66e-02
 i=  80 alpha=1.000 err=0.394 loss=3.23e-02
 i=  96 alpha=1.000 err=0.376 loss=1.84e-02
 i= 112 alpha=1.000 err=0.381 loss=1.01e-02
 i= 128 alpha=1.000 err=0.385 loss=5.53e-03
 i= 144 alpha=1.000 err=0.375 loss=3.09e-03
 i= 160 alpha=1.000 err=0.370 loss=1.66e-03
 i= 176 alpha=1.000 err=0.375 loss=9.14e-04
 i= 192 alpha=1.000 err=0.379 loss=4.97e-04
 i= 208 alpha=1.000 err=0.376 loss=2.70e-04
 i= 224 alpha=1.000 err=0.378 loss=1.51e-04
 i= 240 alpha=1.000 err=0.391 loss=8.35e-05
 i= 256 alpha=1.000 err=0.385 loss=4.56e-05
 i= 272 alpha=1.000 err=0.372 loss=2.54e-05
 i= 288 alpha=1.000 err=0.384 loss=1.37e-05
 i= 304 alpha=1.000 err=0.368 loss=7.41e-06
 i= 320 alpha=1.000 err=0.385 loss=4.00e-06
 i= 336 alpha=1.000 err=0.383 loss=2.12e-06
 i= 352 alpha=1.000 err=0.369 loss=1.13e-06
 i= 368 alpha=1.000 err=0.387 loss=6.21e-07
 i= 384 alpha=1.000 err=0.369 loss=3.37e-07
 i= 400 alpha=1.000 err=0.386 loss=1.87e-07
 i= 416 alpha=1.000 err=0.387 loss=1.04e-07
 i= 432 alpha=1.000 err=0.372 loss=5.66e-08
 i= 448 alpha=1.000 err=0.379 loss=3.05e-08
 i= 464 alpha=1.000 err=0.375 loss=1.64e-08
 i= 480 alpha=1.000 err=0.379 loss=8.63e-09
 i= 496 alpha=1.000 err=0.366 loss=4.78e-09
 i= 512 alpha=1.000 err=0.377 loss=2.56e-09
 i= 528 alpha=1.000 err=0.371 loss=1.37e-09
 i= 544 alpha=1.000 err=0.368 loss=7.20e-10
 i= 560 alpha=1.000 err=0.384 loss=3.84e-10
 i= 576 alpha=1.000 err=0.386 loss=2.05e-10
 i= 592 alpha=1.000 err=0.375 loss=1.11e-10
 i= 608 alpha=1.000 err=0.372 loss=6.01e-11
 i= 624 alpha=1.000 err=0.375 loss=3.27e-11
 i= 640 alpha=1.000 err=0.376 loss=1.78e-11
 i= 656 alpha=1.000 err=0.374 loss=9.38e-12
 i= 672 alpha=1.000 err=0.381 loss=5.09e-12
 i= 688 alpha=1.000 err=0.390 loss=2.78e-12
 i= 704 alpha=1.000 err=0.376 loss=1.54e-12
 i= 720 alpha=1.000 err=0.382 loss=8.19e-13
 i= 736 alpha=1.000 err=0.381 loss=4.53e-13
 i= 752 alpha=1.000 err=0.378 loss=2.44e-13
 i= 768 alpha=1.000 err=0.379 loss=1.34e-13
 i= 784 alpha=1.000 err=0.377 loss=7.10e-14
 i= 800 alpha=1.000 err=0.382 loss=3.96e-14
 i= 816 alpha=1.000 err=0.385 loss=2.14e-14
 i= 832 alpha=1.000 err=0.374 loss=1.14e-14
 i= 848 alpha=1.000 err=0.386 loss=6.20e-15
 i= 864 alpha=1.000 err=0.385 loss=3.37e-15
 i= 880 alpha=1.000 err=0.378 loss=1.81e-15
 i= 896 alpha=1.000 err=0.371 loss=9.68e-16
 i= 912 alpha=1.000 err=0.376 loss=5.10e-16
 i= 928 alpha=1.000 err=0.386 loss=2.79e-16
 i= 944 alpha=1.000 err=0.362 loss=1.51e-16
 i= 960 alpha=1.000 err=0.373 loss=8.31e-17
 i= 976 alpha=1.000 err=0.369 loss=4.32e-17
 i= 992 alpha=1.000 err=0.389 loss=2.32e-17
 i=1008 alpha=1.000 err=0.372 loss=1.25e-17
 i=1024 alpha=1.000 err=0.380 loss=6.87e-18
 i=1040 alpha=1.000 err=0.382 loss=3.74e-18
 i=1056 alpha=1.000 err=0.390 loss=2.03e-18
 i=1072 alpha=1.000 err=0.378 loss=1.08e-18
 i=1088 alpha=1.000 err=0.376 loss=5.81e-19
 i=1104 alpha=1.000 err=0.378 loss=3.15e-19
 i=1120 alpha=1.000 err=0.381 loss=1.74e-19
 i=1136 alpha=1.000 err=0.384 loss=9.43e-20
 i=1152 alpha=1.000 err=0.384 loss=5.05e-20
 i=1168 alpha=1.000 err=0.374 loss=2.70e-20
 i=1184 alpha=1.000 err=0.358 loss=1.42e-20
 i=1200 alpha=1.000 err=0.381 loss=7.70e-21
 i=1216 alpha=1.000 err=0.375 loss=4.14e-21
 i=1232 alpha=1.000 err=0.382 loss=2.27e-21
 i=1248 alpha=1.000 err=0.358 loss=1.21e-21
 i=1264 alpha=1.000 err=0.379 loss=6.40e-22
 i=1280 alpha=1.000 err=0.382 loss=3.47e-22
 i=1296 alpha=1.000 err=0.364 loss=1.89e-22
 i=1312 alpha=1.000 err=0.384 loss=9.98e-23
 i=1328 alpha=1.000 err=0.374 loss=5.38e-23
 i=1344 alpha=1.000 err=0.392 loss=2.90e-23
 i=1360 alpha=1.000 err=0.375 loss=1.57e-23
 i=1376 alpha=1.000 err=0.382 loss=8.30e-24
 i=1392 alpha=1.000 err=0.375 loss=4.39e-24
 i=1408 alpha=1.000 err=0.372 loss=2.36e-24
 i=1424 alpha=1.000 err=0.381 loss=1.26e-24
 i=1440 alpha=1.000 err=0.389 loss=6.71e-25
 i=1456 alpha=1.000 err=0.373 loss=3.48e-25
 i=1472 alpha=1.000 err=0.375 loss=1.90e-25
 i=1488 alpha=1.000 err=0.372 loss=1.00e-25
 i=1504 alpha=1.000 err=0.365 loss=5.48e-26
 i=1520 alpha=1.000 err=0.369 loss=2.90e-26
 i=1536 alpha=1.000 err=0.378 loss=1.56e-26
 i=1552 alpha=1.000 err=0.372 loss=8.30e-27
 i=1568 alpha=1.000 err=0.382 loss=4.48e-27
 i=1584 alpha=1.000 err=0.368 loss=2.49e-27
 i=1600 alpha=1.000 err=0.393 loss=1.37e-27
 i=1616 alpha=1.000 err=0.374 loss=7.50e-28
 i=1632 alpha=1.000 err=0.369 loss=4.14e-28
 i=1648 alpha=1.000 err=0.373 loss=2.19e-28
 i=1664 alpha=1.000 err=0.381 loss=1.19e-28
 i=1680 alpha=1.000 err=0.376 loss=6.33e-29
 i=1696 alpha=1.000 err=0.371 loss=3.40e-29
 i=1712 alpha=1.000 err=0.358 loss=1.81e-29
 i=1728 alpha=1.000 err=0.376 loss=9.76e-30
 i=1744 alpha=1.000 err=0.372 loss=5.25e-30
 i=1760 alpha=1.000 err=0.377 loss=2.82e-30
 i=1776 alpha=1.000 err=0.374 loss=1.53e-30
 i=1792 alpha=1.000 err=0.392 loss=8.17e-31
 i=1808 alpha=1.000 err=0.370 loss=4.37e-31
 i=1824 alpha=1.000 err=0.365 loss=2.28e-31
 i=1840 alpha=1.000 err=0.387 loss=1.20e-31
 i=1856 alpha=1.000 err=0.375 loss=6.41e-32
 i=1872 alpha=1.000 err=0.368 loss=3.47e-32
 i=1888 alpha=1.000 err=0.374 loss=1.83e-32
 i=1904 alpha=1.000 err=0.381 loss=9.64e-33
 i=1920 alpha=1.000 err=0.377 loss=5.21e-33
 i=1936 alpha=1.000 err=0.377 loss=2.78e-33
 i=1952 alpha=1.000 err=0.394 loss=1.51e-33
 i=1968 alpha=1.000 err=0.378 loss=8.09e-34
 i=1984 alpha=1.000 err=0.383 loss=4.40e-34
 i=2000 alpha=1.000 err=0.379 loss=2.42e-34
 i=2016 alpha=1.000 err=0.374 loss=1.30e-34
 i=2032 alpha=1.000 err=0.390 loss=7.06e-35
 i=2048 alpha=1.000 err=0.371 loss=3.77e-35
 i=2064 alpha=1.000 err=0.379 loss=1.97e-35
 i=2080 alpha=1.000 err=0.375 loss=1.04e-35
 i=2096 alpha=1.000 err=0.376 loss=5.63e-36
 i=2112 alpha=1.000 err=0.372 loss=3.00e-36
 i=2128 alpha=1.000 err=0.386 loss=1.61e-36
 i=2144 alpha=1.000 err=0.395 loss=8.70e-37
 i=2160 alpha=1.000 err=0.383 loss=4.67e-37
 i=2176 alpha=1.000 err=0.374 loss=2.53e-37
 i=2192 alpha=1.000 err=0.381 loss=1.40e-37
 i=2208 alpha=1.000 err=0.379 loss=7.59e-38
 i=2224 alpha=1.000 err=0.369 loss=4.05e-38
 i=2240 alpha=1.000 err=0.369 loss=2.15e-38
 i=2256 alpha=1.000 err=0.368 loss=1.16e-38
 i=2272 alpha=1.000 err=0.367 loss=6.23e-39
 i=2288 alpha=1.000 err=0.372 loss=3.39e-39
 i=2304 alpha=1.000 err=0.382 loss=1.88e-39
 i=2320 alpha=1.000 err=0.363 loss=9.97e-40
 i=2336 alpha=1.000 err=0.383 loss=5.42e-40
 i=2352 alpha=1.000 err=0.393 loss=2.97e-40
 i=2368 alpha=1.000 err=0.388 loss=1.59e-40
 stopping early
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=2776.0s).
Done training stage 3 (time=30826s).
---------------------------------------------------------------------------
Done training (time=36644s).
